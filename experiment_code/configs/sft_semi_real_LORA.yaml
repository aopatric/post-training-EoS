experiment_name: sft_semi_real_LORA
ouput_dir: results
seed: 42

model_name: EleutherAI/pythia-70m
use_lora: true
lora_r : 8
lora_alpha : 32
lora_dropout: 0.1

dataset_name : alpaca
max_length: 512
subsample_size: 5000

optimizer: sgd
learning_rate: 1e-4
# using batch GD for this test...
batch_size: 5000
max_steps: 1000
eval_interval: 50
save_interval: 100