use_mlp_dry_run: True
dry_run_mlp_width: 256
dry_run_mlp_depth: 2
model_name: "EleutherAI/pythia-70m" # For tokenizer
dataset_name: "alpaca"
max_steps: 10
eval_interval: 2
batch_size: 4
gradient_accumulation_steps: 1
learning_rate: 1e-3
compute_spectral_sharpness: True
compute_batch_sharpness: True
compute_global_sharpness: True
compute_block_sharpness: True
output_dir: "results/dry_run"
experiment_name: "mlp_dry_run_test"
device: "cpu" # Force CPU for simple test to avoid CUDA issues in environment if any
