experiment_name: Full Param SFT, 70M Params, Alpaca

use_wandb: true
wandb_project: post_training_eos
wandb_run_name: sft_fullparam_demo
wandb_group: sft_fullparam

model_name: EleutherAI/pythia-70m
use_lora: false

dataset_name: alpaca
max_length: 2048
subsample_size: 20000

batch_size: 1
optimizer: sgd
learning_rate: [0.001, 0.01, 0.1, 1]
weight_decay: 1e-4
momentum: 0.9

eval_interval: 1000
save_interval: 5000
max_steps: 10000
device: cuda

gradient_checkpointing: false
compute_global_sharpness: false
compute_block_sharpness: true
sharpness_layer_indices: [-1]
