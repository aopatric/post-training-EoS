# main
experiment_name: Full Param SFT, 70M Params, Alpaca, BATCH SHARPNESS
seed: 42

# wandb
use_wandb: true
wandb_project: post_training_eos
wandb_run_name: sft_lora
wandb_group: sft_lora

# model
model_name: EleutherAI/pythia-70m
use_lora: true

lora_r: 8
lora_alpha: 32
lora_dropout: 0.1

# data
dataset_name: alpaca
max_length: 1024
subsample_size: 50000

# optimization
batch_size: 4
gradient_accumulation_steps: 4
optimizer: sgd
learning_rate: [5e-5, 1e-4, 5e-4, 1e-3]
max_grad_norm: 10.0
weight_decay: 1e-5
momentum: 0.9

# saving, device, short experiment for now since we need to get graphs!
eval_interval: 100
save_interval: 5000
max_steps: 50000
device: cuda

# EoS Stuff
compute_spectral_sharpness: false
compute_batch_sharpness: true

compute_global_sharpness: true
compute_block_sharpness: false
sharpness_layer_indices: [-1]

# misc
gradient_checkpointing: false
